# Docker Registry Configuration
# Leave empty for Docker Hub, or set to your private registry
# Examples:
# DOCKER_REGISTRY=
# DOCKER_REGISTRY=my-registry.com/
# DOCKER_REGISTRY=registry.gitlab.com/myproject/
DOCKER_REGISTRY=

# Docker Build Target: dev or prod
# dev  = Hot reload, root user, all dependencies
# prod = Production optimized, appuser, no-dev dependencies
BUILD_TARGET=dev

# Image versions
# https://hub.docker.com/r/grafana/grafana/tags
IMG_GRAFANA=grafana/grafana:12.2
# https://hub.docker.com/r/mcp/grafana/tags
IMG_GRAFANA_MCP=mcp/grafana:latest
# https://hub.docker.com/r/grafana/loki/tags
IMG_LOKI=grafana/loki:3.5.8
# https://hub.docker.com/r/grafana/tempo/tags
IMG_TEMPO=grafana/tempo:2.9.0
# https://hub.docker.com/r/grafana/mimir/tags
IMG_MIMIR=grafana/mimir:3.0.0
# https://hub.docker.com/r/otel/opentelemetry-collector-contrib/tags
IMG_OTEL=otel/opentelemetry-collector-contrib:0.139.0
# https://hub.docker.com/_/postgres/tags
IMG_POSTGRES=postgres:18
# https://hub.docker.com/_/adminer/tags
IMG_ADMINER=adminer:5.4.1
# https://hub.docker.com/r/confluentinc/cp-kafka/tags
IMG_KAFKA=confluentinc/cp-kafka:8.1.0
# https://hub.docker.com/r/tchiotludo/akhq/tags
IMG_AKHQ=tchiotludo/akhq:0.26.0

# Base images
IMG_PYTHON=python:3.14-slim

# Docker Compose optimization settings
COMPOSE_PARALLEL_LIMIT=8
COMPOSE_HTTP_TIMEOUT=120
DOCKER_BUILDKIT=1
COMPOSE_DOCKER_CLI_BUILD=1

# Grafana Token
GRAFANA_SERVICE_ACCOUNT_TOKEN=<your_grafana_service_account_token>

# Agentic Network - LLM Configuration
# LLM endpoint using Docker Model Runner (ai/qwen3 model)
# The default points to the Docker host's local AI service
LLM_BASE_URL=http://172.17.0.1:12434/v1
LLM_API_KEY=not-needed

# Maximum characters to allow in prompts sent to the LLM. Set according to your model's
# context window and the runtime limits of your model-runner. Default used in code: 9000
LLM_MAX_PROMPT_CHARS=9000
# If true, instantiate a fresh LLM client per call to avoid preserving conversation
# state between microservice calls. This helps prevent unintended context stacking.
# Set to 'true' or 'false'. Default: false
LLM_EPHEMERAL_PER_CALL=false

# Timeout (seconds) used by the UI when calling the orchestrator. Increase if
# orchestration and LLM synthesis can take longer. Default: 120
ORCHESTRATOR_TIMEOUT=120

# Per-agent call timeout used by orchestrator when calling specialized agents
# (logs/metrics/traces). Default: 60
AGENT_CALL_TIMEOUT=60
