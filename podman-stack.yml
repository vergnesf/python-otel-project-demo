apiVersion: v1
---
kind: ConfigMap
metadata:
  name: loki-config
data:
  loki-config.yml: |
    auth_enabled: false
    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
    common:
      instance_addr: 127.0.0.1
      path_prefix: /tmp/loki
      storage:
        filesystem:
          chunks_directory: /tmp/loki/chunks
          rules_directory: /tmp/loki/rules
      replication_factor: 1
      ring:
        kvstore:
          store: inmemory
    schema_config:
      configs:
        - from: 2020-10-24
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h
    ruler:
      alertmanager_url: http://localhost:9093
    frontend:
      encoding: protobuf

---
kind: ConfigMap
metadata:
  name: mimir-config
data:
  mimir-config.yml: |
    multitenancy_enabled: false
    activity_tracker: {}
    alertmanager: {}
    alertmanager_storage:
      backend: local
    server:
      http_listen_port: 9009
      grpc_server_max_recv_msg_size: 104857600
      grpc_server_max_send_msg_size: 104857600
      grpc_server_max_concurrent_streams: 1000
    distributor:
      pool:
        health_check_ingesters: true
    ingester_client:
      grpc_client_config:
        grpc_compression: gzip
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
    ingester:
      ring:
        final_sleep: 0s
        kvstore:
          store: inmemory
        min_ready_duration: 0s
        num_tokens: 512
        replication_factor: 1
    blocks_storage:
      backend: filesystem
      bucket_store:
        sync_dir: /tmp/mimir/tsdb-sync
      filesystem:
        dir: /tmp/mimir/blocks
      tsdb:
        dir: /tmp/mimir/tsdb
    compactor:
      sharding_ring:
        kvstore:
          store: inmemory
    ruler:
      enable_api: true
    ruler_storage:
      backend: filesystem
      local:
        directory: /tmp/mimir/rules
    limits:
      ingestion_burst_size: 500000
      ingestion_rate: 250000
      max_global_exemplars_per_user: 100000
---
kind: ConfigMap
metadata:
  name: otel-conf
data:
  otel-conf.yml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318
          grpc:
            endpoint: 0.0.0.0:4317
    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 30
      batch:
        send_batch_size: 4096
        timeout: 10s
    exporters:
      otlphttp/mimir:
        endpoint: http://mimir:9009/otlp
      otlphttp/loki:
        endpoint: "http://loki:3100/otlp"
      otlphttp/tempo:
        endpoint: "http://tempo:4318"
    extensions:
      health_check:
    service:
      extensions: [health_check]
      pipelines:
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlphttp/mimir]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlphttp/loki]
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlphttp/tempo]
---
kind: ConfigMap
metadata:
  name: tempo-config
data:
  tempo.yml: |
    server:
      http_listen_port: 3200
      grpc_listen_port: 9097
    distributor:
      receivers: # this configuration will listen on all ports and protocols that tempo is capable of.
        otlp:
          protocols:
            http:
              endpoint: 0.0.0.0:4318
    ingester:
      trace_idle_period: 10s # the length of time after a trace has not received spans to consider it complete and flush it
      max_block_bytes: 1_000_000 # cut the head block when it hits this size or ...
      max_block_duration: 5m #   this much time passes
    compactor:
      compaction:
        compaction_window: 1h # blocks in this time window will be compacted together
        max_block_bytes: 100_000_000 # maximum size of compacted blocks
        block_retention: 1h
        compacted_block_retention: 10m
    metrics_generator:
      processor:
        service_graphs:
          histogram_buckets: [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8]
          dimensions:
            - job
            - service.name
            - service.namespace
            - service.instance.id
            - host
            - environment
            - net.k8s.name
            - net.peer.name
            - net.host.name
            - http.method
            - http.status_code
            - db.name
            - http.url
            - http.host
            - http.route
            - http.method
            - code.function
            - code.namespace
            - db.operation
            - db.sql.table
            - otel.library.version
        span_metrics:
          histogram_buckets:
            [
              0.002,
              0.004,
              0.008,
              0.016,
              0.032,
              0.064,
              0.128,
              0.256,
              0.512,
              1.02,
              2.05,
              4.10,
            ]
          dimensions:
            - job
            - service.name
            - service.namespace
            - service.instance.id
            - host
            - environment
            - net.k8s.name
            - net.peer.name
            - net.host.name
            - http.method
            - http.status_code
            - db.name
            - http.url
            - http.host
            - http.route
            - http.method
            - code.function
            - code.namespace
            - db.operation
            - db.sql.table
            - otel.library.version
          enable_target_info: false
      storage:
        path: /tmp/tempo/generator
        remote_write:
          - url: "http://mimir:9009/api/v1/push"
            send_exemplars: true
    storage:
      trace:
        backend: local # backend configuration to use
        block:
          bloom_filter_false_positive: .05 # bloom filter false positive rate.  lower values create larger filters but fewer false positives
          v2_index_downsample_bytes: 1000 # number of bytes per index record
          v2_encoding: zstd # block encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd, s2
        wal:
          path: /tmp/tempo/wal # where to store the the wal locally
          v2_encoding: snappy # wal encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd, s2
        local:
          path: /tmp/tempo/blocks
        pool:
          max_workers: 100 # worker pool determines the number of parallel requests to the object store backend
          queue_depth: 10000
    overrides:
      defaults:
        metrics_generator:
          processors:
            - service-graphs
            - span-metrics
---
kind: ConfigMap
metadata:
  name: grafana-datasources
data:
  default.yaml: |
    apiVersion: 1
    datasources:
      - name: mimir
        uid: mymimir
        type: prometheus
        url: http://mimir:9009/prometheus
        jsonData:
          exemplarTraceIdDestinations:
            - datasourceUid: mytempo
              name: trace_id

      - name: tempo
        uid: mytempo
        type: tempo
        url: http://tempo:3200
        jsonData:
          tracesToLogsV2:
            datasourceUid: myloki
            spanStartTimeShift: "1h"
            spanEndTimeShift: "-1h"
            tags: [{ key: "service.name", value: "application" }]
            filterByTraceID: true
            filterBySpanID: true
            customQuery: false
          tracesToMetrics:
            datasourceUid: mymimir
            spanStartTimeShift: "1h"
            spanEndTimeShift: "-1h"
            tags: [{ key: "service.name", value: "application" }]
            queries:
              - name: "Sample query"
                query: "sum(rate(duration_milliseconds_bucket{$$__tags}[5m]))"
          serviceMap:
            datasourceUid: mymimir
          nodeGraph:
            enabled: true
          lokiSearch:
            datasourceUid: myloki

      - name: loki
        uid: myloki
        type: loki
        url: http://loki:3100
        jsonData:
          derivedFields:
            - name: "traceID"
              matcherRegex: '((?<=traceid":")[A-Za-z0-9]+)'
              url: "$${__value.raw}"
              datasourceUid: mytempo
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    bind-mount-options: /home/fabien/workspace/git/vergnesf/python-otel-project-demo/config/mimir-config.yml:Z
    io.kubernetes.cri-o.SandboxID/adminer: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/akhq: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/broker: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/customer: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/grafana: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/loki: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/mimir: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/order: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/ordercheck: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/ordermanagement: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/otel: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/postgres: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/stock: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/supplier: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/suppliercheck: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/tempo: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
    io.kubernetes.cri-o.SandboxID/zookeeper: 283dbae40e4eaad499d0bac44cf6f31a1361d60c277426bc928997b4e09ff21e
  creationTimestamp: "2025-08-30T14:47:50Z"
  labels:
    app: otel-python-demo
  name: otel-python-demo
spec:
  containers:
    - env:
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: Admin
        - name: GF_FEATURE_TOGGLES_ENABLE
          value: tempoServiceGraph,traceToMetrics
        - name: GF_AUTH_DISABLE_LOGIN_FORM
          value: "true"
      image: docker.io/grafana/grafana:12.1.1
      ports:
        - containerPort: 3000
          hostPort: 3000
      name: grafana
      securityContext:
        runAsNonRoot: true
      volumeMounts:
        - name: grafana-datasources
          mountPath: /etc/grafana/provisioning/datasources
        - mountPath: /var/lib/grafana
          name: grafana-data-pvc
    #    - env:
    #        - name: ZOOKEEPER_CLIENT_PORT
    #          value: "2181"
    #        - name: ZOOKEEPER_TICK_TIME
    #          value: "2000"
    #      image: docker.io/confluentinc/cp-zookeeper:7.9.2
    #      name: zookeeper
    #      securityContext: {}
    #      volumeMounts:
    #        - mountPath: /etc/zookeeper/secrets
    #          name: zookeeper-secret-pvc-pvc
    #        - mountPath: /var/lib/zookeeper/data
    #          name: zookeeper-data-pvc
    #        - mountPath: /var/lib/zookeeper/log
    #          name: zookeeper-log-pvc
    - env:
        - name: KAFKA_NODE_ID
          value: "1"
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS
          value: "0"
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "1"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_JMX_PORT
          value: "9101"
        - name: KAFKA_JMX_HOSTNAME
          value: localhost
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        - name: KAFKA_CONTROLLER_QUORUM_VOTERS
          value: "1@broker:29093"
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"
        - name: KAFKA_LOG_DIRS
          value: "/tmp/kraft-combined-logs"
        - name: CLUSTER_ID
          value: "jm1kaYDGRR2EDm8ZaQT84w"
      image: docker.io/confluentinc/cp-kafka:8.0.0
      name: broker
      securityContext: {}
      volumeMounts:
        - mountPath: /etc/kafka/secrets
          name: kafka-secret-pvc
        - mountPath: /var/lib/kafka/data
          name: kafka-data-pvc
    - args:
        - postgres
      env:
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          value: yourpassword
        - name: POSTGRES_DB
          value: mydatabase
      image: docker.io/library/postgres:17
      name: postgres
      volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: postgres-data-pvc
    - env:
        - name: INTERVAL_SECONDS
          value: "5"
        - name: OTEL_LOGS_EXPORTER
          value: otlp
        - name: LOG_LEVEL
          value: DEBUG
        - name: ERROR_RATE
          value: "0.1"
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: broker:9092
        - name: OTEL_SERVICE_NAME
          value: customer
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
        - name: OTEL_METRICS_EXPORTER
          value: otlp
      image: localhost/customer:latest
      name: customer
      securityContext: {}
    - env:
        - name: LOG_LEVEL
          value: DEBUG
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: INTERVAL_SECONDS
          value: "5"
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_LOGS_EXPORTER
          value: otlp
        - name: ERROR_RATE
          value: "0.1"
        - name: OTEL_SERVICE_NAME
          value: supplier
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: broker:9092
        - name: OTEL_METRICS_EXPORTER
          value: otlp
      image: localhost/supplier:latest
      name: supplier
      securityContext: {}
    - env:
        - name: OTEL_SERVICE_NAME
          value: order
        - name: ERROR_RATE
          value: "0.1"
        - name: OTEL_LOGS_EXPORTER
          value: otlp
        - name: DATABASE_URL
          value: postgresql://postgres:yourpassword@postgres:5432/mydatabase
        - name: LOG_LEVEL
          value: DEBUG
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: OTEL_METRICS_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
      image: localhost/order:latest
      name: order
      securityContext: {}
    - env:
        - name: OTEL_SERVICE_NAME
          value: stock
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: LOG_LEVEL
          value: DEBUG
        - name: OTEL_LOGS_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
        - name: ERROR_RATE
          value: "0.1"
        - name: DATABASE_URL
          value: postgresql://postgres:yourpassword@postgres:5432/mydatabase
        - name: OTEL_METRICS_EXPORTER
          value: otlp
      image: localhost/stock:latest
      name: stock
      securityContext: {}
    - env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: broker:9092
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: API_URL
          value: http://order:5000
        - name: OTEL_SERVICE_NAME
          value: ordercheck
        - name: ERROR_RATE
          value: "0.1"
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
        - name: LOG_LEVEL
          value: DEBUG
        - name: OTEL_METRICS_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_LOGS_EXPORTER
          value: otlp
      image: localhost/ordercheck:latest
      name: ordercheck
      securityContext: {}
    - env:
        - name: OTEL_LOGS_EXPORTER
          value: otlp
        - name: LOG_LEVEL
          value: DEBUG
        - name: ERROR_RATE
          value: "0.1"
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
        - name: OTEL_SERVICE_NAME
          value: suppliercheck
        - name: OTEL_METRICS_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: API_URL
          value: http://stock:5000
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: broker:9092
      image: localhost/suppliercheck:latest
      name: suppliercheck
      securityContext: {}
    - env:
        - name: OTEL_LOGS_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
          value: "true"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel:4317
        - name: ERROR_RATE
          value: "0.1"
        - name: OTEL_METRICS_EXPORTER
          value: otlp
        - name: OTEL_PYTHON_LOG_CORRELATION
          value: "true"
        - name: OTEL_TRACES_EXPORTER
          value: otlp
        - name: API_URL_STOCKS
          value: http://stock:5000
        - name: INTERVAL_SECONDS
          value: "5"
        - name: LOG_LEVEL
          value: DEBUG
        - name: OTEL_SERVICE_NAME
          value: ordermanagement
        - name: API_URL_ORDERS
          value: http://order:5000
      image: localhost/ordermanagement:latest
      name: ordermanagement
      securityContext: {}
    - args:
        - --config=/etc/otel-conf.yml
      image: docker.io/otel/opentelemetry-collector-contrib:0.133.0
      name: otel
      securityContext: {}
      volumeMounts:
        - mountPath: /etc/otel-conf.yml
          name: otel-conf
          subPath: otel-conf.yml
    - args:
        - -config.file=/etc/config/loki.yaml
      image: docker.io/grafana/loki:3.5.3
      name: loki
      securityContext:
        runAsGroup: 0
        runAsNonRoot: true
        runAsUser: 0
      volumeMounts:
        - mountPath: /etc/config/loki.yaml
          name: loki-config
          subPath: loki-config.yml
        - mountPath: /tmp/loki
          name: loki-data-pvc
    - args:
        - -config.file=/etc/tempo.yaml
      image: docker.io/grafana/tempo:2.8.2
      name: tempo
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      volumeMounts:
        - mountPath: /etc/tempo.yaml
          name: tempo-config
          subPath: tempo.yml
        - mountPath: /tmp/tempo
          name: tempo-data-pvc
    - args:
        - -config.file=/etc/mimir-config.yml
      image: docker.io/grafana/mimir:2.17.0
      name: mimir
      securityContext: {}
      volumeMounts:
        - mountPath: /etc/mimir-config.yml
          name: mimir-config
          subPath: mimir-config.yml
        - mountPath: /tmp/mimir
          name: mimir-data-pvc
    - args:
        - php
        - -S
        - "[::]:8080"
        - -t
        - /var/www/html
      image: docker.io/library/adminer:5.3.0
      name: adminer
      ports:
    #        - containerPort: 8081
    #          hostPort: 8081
    #      securityContext: {}
    - args:
        - ./akhq
      env:
        - name: AKHQ_CONFIGURATION
          value: |-
            micronaut:
              server:
                port: 8081
            akhq:
              connections:
                docker-kafka-server:
                  properties:
                    bootstrap.servers: "broker:9092"
      image: docker.io/tchiotludo/akhq:0.26.0
      #      ports:
      #        - containerPort: 8080
      #          hostPort: 8080
      name: akhq
      securityContext: {}
  volumes:
    - name: grafana-datasources
      configMap:
        name: grafana-datasources
    - name: loki-config
      configMap:
        name: loki-config
    - name: mimir-config
      configMap:
        name: mimir-config
    - name: otel-conf
      configMap:
        name: otel-conf
    - name: tempo-config
      configMap:
        name: tempo-config
    - name: grafana-data-pvc
      persistentVolumeClaim:
        claimName: grafana-data
    - name: kafka-secret-pvc
      persistentVolumeClaim:
        claimName: kafka-secret
    - name: kafka-data-pvc
      persistentVolumeClaim:
        claimName: kafka-data
    - name: postgres-data-pvc
      persistentVolumeClaim:
        claimName: postgres-data
    - name: loki-data-pvc
      persistentVolumeClaim:
        claimName: loki-data
    - name: tempo-data-pvc
      persistentVolumeClaim:
        claimName: tempo-data
    - name: mimir-data-pvc
      persistentVolumeClaim:
        claimName: mimir-data
    - name: zookeeper-secret-pvc-pvc
      persistentVolumeClaim:
        claimName: zookeeper-secret-pvc
    - name: zookeeper-data-pvc
      persistentVolumeClaim:
        claimName: zookeeper-data
    - name: zookeeper-log-pvc
      persistentVolumeClaim:
        claimName: zookeeper-log
