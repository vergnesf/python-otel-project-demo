#!/usr/bin/env python3
"""
Integration tests for the simplified Orchestrator
Tests the key functionalities with real HTTP calls:
1. Agent routing (logs, traces, metrics)
2. Response validation

REQUIREMENTS:
- Orchestrator must be running on http://localhost:8001
- LLM must be running at http://172.17.0.1:12434/v1 (docker-compose default)
- All tests require a working LLM for routing and validation
"""

import asyncio
import os
from datetime import datetime

import httpx
import pytest


class ExecutionResult:
    """Structure for test execution results"""

    def __init__(self, status: str = "NOT_RUN", warnings: list[str] | None = None):
        self.status = status  # "COMPLETED", "ERROR", "WARNING", "NOT_RUN"
        self.warnings = warnings or []

    def add_warning(self, warning: str):
        self.warnings.append(warning)
        if self.status == "NOT_RUN":
            self.status = "WARNING"

    def set_error(self, error: str):
        self.status = "ERROR"
        self.warnings = [error]


BASE_URL = "http://localhost:8001"
TIMEOUT = 120.0

# LLM URL (default to local Ollama OpenAI-compatible endpoint)
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://localhost:11434/v1")


class Colors:
    """ANSI color codes for terminal output"""

    GREEN = "\033[92m"
    RED = "\033[91m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    BOLD = "\033[1m"
    END = "\033[0m"


def _ollama_tags_url(base_url: str) -> str:
    """Derive the Ollama /api/tags URL from a base URL."""
    normalized = base_url.rstrip("/")
    if normalized.endswith("/v1"):
        return f"{normalized[:-3]}/api/tags"
    if normalized.endswith("/api"):
        return f"{normalized}/tags"
    return f"{normalized}/api/tags"


async def resolve_available_model() -> str | None:
    """Resolve an available model name from Ollama or skip tests if none found."""
    configured = os.getenv("LLM_MODEL")
    if configured:
        return configured

    tags_url = _ollama_tags_url(LLM_BASE_URL)
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(tags_url)
            response.raise_for_status()
            data = response.json()
    except Exception as exc:
        pytest.skip(f"Unable to query Ollama models at {tags_url}: {exc}")

    models = data.get("models", []) if isinstance(data, dict) else []
    if not models:
        pytest.skip("No Ollama models found. Run `make models-init` before tests.")

    model_name = models[0].get("name") if isinstance(models[0], dict) else None
    if not model_name:
        pytest.skip("Ollama models list is missing names.")

    return model_name


async def list_available_models() -> list[str]:
    """List available Ollama model names (OpenAI-compatible endpoint)."""
    tags_url = _ollama_tags_url(LLM_BASE_URL)
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(tags_url)
            response.raise_for_status()
            data = response.json()
    except Exception as exc:
        pytest.skip(f"Unable to query Ollama models at {tags_url}: {exc}")

    models = data.get("models", []) if isinstance(data, dict) else []
    if not models:
        pytest.skip("No Ollama models found. Run `make models-init` before tests.")

    names = [m.get("name") for m in models if isinstance(m, dict) and m.get("name")]
    if not names:
        pytest.skip("Ollama models list is missing names.")

    return names


async def check_orchestrator_available():
    """Check if orchestrator is running"""
    print(f"Checking orchestrator availability at {BASE_URL}/health...")
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{BASE_URL}/health")
            if response.status_code != 200:
                print(f"Orchestrator returned status {response.status_code}")
                pytest.skip(
                    f"Orchestrator not healthy at {BASE_URL} (status: {response.status_code})"
                )
            print("Orchestrator is available.")
    except Exception as e:
        print(f"Failed to connect to orchestrator: {e}")
        pytest.skip(f"Orchestrator not running at {BASE_URL}: {e}")


async def run_agent_routing(
    model: str | None = None,
    *,
    strict: bool = True,
    model_params: dict | None = None,
):
    """
    Reusable logic for Agent Routing

    Args:
        model: Model name to use
        strict: If True, raise assertions on failure; if False, print warnings
        model_params: Optional LLM parameters (temperature, top_k, max_tokens)
    """
    await check_orchestrator_available()

    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.END}")
    print(f"{Colors.BOLD}TEST 1: AGENT ROUTING (Model: {model}){Colors.END}")
    print(f"{Colors.BOLD}{'=' * 80}{Colors.END}\n")

    test_cases = [
        {
            "query": "Show me recent errors",
            "expected_agents": ["logs"],
            "description": "Error query should route to logs agent",
        },
        {
            "query": "What is the CPU usage?",
            "expected_agents": ["metrics"],
            "description": "CPU query should route to metrics agent",
        },
        {
            "query": "Which requests are slow?",
            "expected_agents": ["traces"],
            "description": "Slow query should route to traces agent",
        },
        {
            "query": "Show errors and CPU usage",
            "expected_agents": ["logs", "metrics"],
            "description": "Complex query should route to multiple agents",
        },
    ]

    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        for i, test_case in enumerate(test_cases, 1):
            query = test_case["query"]
            print(f"{Colors.BLUE}Test {i}: '{query}'{Colors.END}")
            print(f"  Description: {test_case['description']}")

            payload = {"query": query, "time_range": "1h"}
            if model:
                payload["model"] = model
            if model_params:
                payload["model_params"] = model_params

            response = await client.post(f"{BASE_URL}/analyze", json=payload)

            assert response.status_code == 200, f"HTTP {response.status_code}"

            data = response.json()
            routing = data.get("routing", {})
            agents_called = routing.get("agents", [])
            reason = routing.get("reason", "")

            print(f"  Agents called: {Colors.YELLOW}{agents_called}{Colors.END}")
            print(f"  Reason: {Colors.YELLOW}{reason}{Colors.END}")

            # Verify that at least one expected agent was called
            has_expected = any(
                agent in agents_called for agent in test_case["expected_agents"]
            )
            if strict:
                assert (
                    has_expected
                ), f"Expected one of {test_case['expected_agents']}, got {agents_called}"
            elif not has_expected:
                print(
                    f"  {Colors.YELLOW}‚ö†Ô∏è  Expected one of {test_case['expected_agents']}, got {agents_called}{Colors.END}"
                )

            print(f"  {Colors.GREEN}‚úì Test {i} passed{Colors.END}\n")
            await asyncio.sleep(1)

    print(f"{Colors.GREEN}{Colors.BOLD}‚úì All routing tests passed!{Colors.END}\n")


async def test_agent_routing():
    """
    TEST 1: Agent Routing
    Verify that queries are routed to the correct agents
    """
    model = await resolve_available_model()
    await run_agent_routing(model=model)


async def run_response_validation(
    model: str | None = None,
    *,
    strict: bool = True,
    model_params: dict | None = None,
):
    """
    Reusable logic for Response Validation
    """
    await check_orchestrator_available()

    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.END}")
    print(f"{Colors.BOLD}TEST 2: RESPONSE VALIDATION (Model: {model}){Colors.END}")
    print(f"{Colors.BOLD}{'=' * 80}{Colors.END}\n")

    test_cases = [
        {"query": "Show me recent errors", "description": "Simple error query"},
        {"query": "What is the system health?", "description": "Complex health query"},
    ]

    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        for i, test_case in enumerate(test_cases, 1):
            query = test_case["query"]
            print(f"{Colors.BLUE}Test {i}: '{query}'{Colors.END}")
            print(f"  Description: {test_case['description']}")

            payload = {"query": query, "time_range": "1h"}
            if model:
                payload["model"] = model
            if model_params:
                payload["model_params"] = model_params

            response = await client.post(f"{BASE_URL}/analyze", json=payload)

            assert response.status_code == 200, f"HTTP {response.status_code}"

            data = response.json()
            validation = data.get("validation", {})

            print(
                f"  Validation present: {Colors.YELLOW}{'validation' in data}{Colors.END}"
            )

            if validation:
                validated = validation.get("validated", False)
                issues = validation.get("issues", [])
                suggestion = validation.get("suggestion", "")

                print(f"  Validated: {Colors.YELLOW}{validated}{Colors.END}")
                print(f"  Issues: {Colors.YELLOW}{issues}{Colors.END}")
                print(f"  Suggestion: {Colors.YELLOW}{suggestion}{Colors.END}")

                # Validation should have been attempted
                if strict:
                    assert (
                        "validated" in validation or "reason" in validation
                    ), "Validation result should be present"

            print(f"  {Colors.GREEN}‚úì Test {i} passed{Colors.END}\n")
            await asyncio.sleep(1)

    print(f"{Colors.GREEN}{Colors.BOLD}‚úì All validation tests passed!{Colors.END}\n")


async def test_response_validation():
    """
    TEST 2: Response Validation
    Verify that agent responses are validated
    """
    model = await resolve_available_model()
    await run_response_validation(model=model)


async def run_complete_workflow(
    model: str | None = None,
    *,
    strict: bool = True,
    model_params: dict | None = None,
):
    """
    Reusable logic for Complete Workflow

    Args:
        model: Model name to use
        strict: If True, raise assertions on failure; if False, print warnings
        model_params: Optional LLM parameters (temperature, top_k, max_tokens)
    """
    await check_orchestrator_available()

    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.END}")
    print(
        f"{Colors.BOLD}TEST 3: COMPLETE WORKFLOW (Routing ‚Üí Validation) (Model: {model}){Colors.END}"
    )
    print(f"{Colors.BOLD}{'=' * 80}{Colors.END}\n")

    query = "Show me recent errors for the customer service"
    print(f"{Colors.BLUE}Query: '{query}'{Colors.END}\n")

    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        payload = {"query": query, "time_range": "1h"}
        if model:
            payload["model"] = model
        if model_params:
            payload["model_params"] = model_params

        response = await client.post(f"{BASE_URL}/analyze", json=payload)

        assert response.status_code == 200, f"HTTP {response.status_code}"

        data = response.json()

        # Step 1: Verify routing
        print(f"{Colors.BOLD}1Ô∏è‚É£ ROUTING:{Colors.END}")
        routing = data.get("routing", {})
        agents = routing.get("agents", [])
        reason = routing.get("reason", "")

        print(f"  Agents: {Colors.YELLOW}{agents}{Colors.END}")
        print(f"  Reason: {Colors.YELLOW}{reason}{Colors.END}")

        if strict:
            assert len(agents) > 0, "At least one agent should be called"
        print(f"  {Colors.GREEN}‚úì Routing OK{Colors.END}\n")

        # Step 2: Verify agent responses
        print(f"{Colors.BOLD}2Ô∏è‚É£ AGENT RESPONSES:{Colors.END}")
        agent_responses = data.get("agent_responses", {})

        for agent_name, agent_resp in agent_responses.items():
            if agent_resp and isinstance(agent_resp, dict):
                if "error" in agent_resp:
                    print(
                        f"  {Colors.RED}‚ùå {agent_name}: {agent_resp['error']}{Colors.END}"
                    )
                else:
                    analysis = agent_resp.get("analysis", "")[:100]
                    print(f"  {Colors.GREEN}‚úì {agent_name}: {analysis}...{Colors.END}")

        if strict:
            assert len(agent_responses) > 0, "Should have agent responses"
        print(f"  {Colors.GREEN}‚úì Agents responded{Colors.END}\n")

        # Step 3: Verify validation
        print(f"{Colors.BOLD}3Ô∏è‚É£ VALIDATION:{Colors.END}")
        validation = data.get("validation", {})

        if validation:
            validated = validation.get("validated", False)
            print(f"  Validated: {Colors.YELLOW}{validated}{Colors.END}")

            if "issues" in validation:
                issues = validation["issues"]
                print(f"  Issues: {Colors.YELLOW}{issues}{Colors.END}")

            if "suggestion" in validation:
                suggestion = validation["suggestion"]
                print(f"  Suggestion: {Colors.YELLOW}{suggestion}{Colors.END}")

        print(f"  {Colors.GREEN}‚úì Validation present{Colors.END}\n")

        # Step 4: Verify summary
        print(f"{Colors.BOLD}4Ô∏è‚É£ SUMMARY:{Colors.END}")
        summary = data.get("summary", "")
        recommendations = data.get("recommendations", [])

        print(f"  Summary ({len(summary)} chars): {summary[:150]}...")
        print(f"  Recommendations: {len(recommendations)} items")

        if strict:
            assert len(summary) > 0, "Summary should not be empty"
        print(f"  {Colors.GREEN}‚úì Summary OK{Colors.END}\n")

    print(f"{Colors.GREEN}{Colors.BOLD}‚úì Complete workflow successful!{Colors.END}\n")


async def test_complete_workflow():
    """
    TEST 4: Complete Workflow
    Test all 3 functionalities together in a single request
    """
    model = await resolve_available_model()
    await run_complete_workflow(model=model)


async def main():
    """Run all integration tests"""
    start_time = datetime.now()

    print(f"\n{Colors.BOLD}‚ïî{'=' * 78}‚ïó{Colors.END}")
    print(
        f"{Colors.BOLD}‚ïë{' ' * 15}INTEGRATION TESTS - SIMPLIFIED ORCHESTRATOR{' ' * 20}‚ïë{Colors.END}"
    )
    print(f"{Colors.BOLD}‚ïö{'=' * 78}‚ïù{Colors.END}")

    try:
        # Test 1: Agent routing
        await test_agent_routing()

        # Test 2: Response validation
        await test_response_validation()

        # Test 3: Complete workflow
        await test_complete_workflow()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"\n{Colors.BOLD}{'=' * 80}{Colors.END}")
        print(f"{Colors.GREEN}{Colors.BOLD}‚úì ALL TESTS PASSED!{Colors.END}")
        print(f"{Colors.YELLOW}‚è±Ô∏è  Total duration: {duration:.2f} seconds{Colors.END}")
        print(f"{Colors.BOLD}{'=' * 80}{Colors.END}\n")

        print(f"{Colors.BOLD}üìä VERIFIED FUNCTIONALITIES:{Colors.END}")
        print(
            f"   {Colors.GREEN}‚úì Intelligent routing to correct agents (logs/metrics/traces){Colors.END}"
        )
        print(f"   {Colors.GREEN}‚úì AI-powered response validation{Colors.END}")
        print(f"   {Colors.GREEN}‚úì Complete end-to-end workflow{Colors.END}\n")

        return 0

    except AssertionError as e:
        print(f"\n{Colors.RED}{Colors.BOLD}‚ùå TEST FAILED:{Colors.END} {e}\n")
        return 1
    except Exception as e:
        print(f"\n{Colors.RED}{Colors.BOLD}‚ùå ERROR:{Colors.END} {e}\n")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
